:PROPERTIES:
:ID:       53b3dadd-acc8-4431-a473-3f450f0e766b
:END:
#+title: linear transformation

* Definition

A linear transformation $T$ *takes vectors $v$ to vectors $T(v)$*.

In other words, a *linear map* is a mapping $V \rightarrow W$ between two vector spaces that *preserves the operations of vector addition and scalar multiplication*.

Linearity requires $T(cv + dw) = cT(v) + dT(w)$ (associativity and homogeneity).

Similar to a function, $v$ as input, $T(v) = Av$ as output. *We are transforming the whole space $V$* when we multiply every $v$ by $A$ (A matrix $A$ transforms $v$ to $Av$).

** Invertibility

A nonlinear transformation $T$ is invertible if every $b$ in the output space comes from exactly one $x$ in the input space: *$T(x) = b$ always has exactly one solution*.

** Subspaces

For a matrix, the *column space contains all outputs $Av$*. The *nullspace contains all inputs for which $Av = 0$*:

#+DOWNLOADED: screenshot @ 2023-05-01 10:36:25
[[file:2023-05-01_10-36-25_screenshot.png]]

* Examples


#+DOWNLOADED: screenshot @ 2023-05-01 09:01:50
[[file:Examples/2023-05-01_09-01-50_screenshot.png]]

* Linearity of transformations

Every point on the input line goes onto the output line:

#+DOWNLOADED: screenshot @ 2023-05-01 09:10:00
[[file:Linearity_of_transformations/2023-05-01_09-10-00_screenshot.png]]

Equally spaced points go to equally spaced points.

With $v_1,v_2,v_3$, the input triangle goes onto the output triangle:

#+DOWNLOADED: screenshot @ 2023-05-01 09:11:10
[[file:Linearity_of_transformations/2023-05-01_09-11-10_screenshot.png]]

If you know $T(v)$ for all vectors $v_1,\cdot \cdot \cdot,v_n$ in a basis, then *you know $T(u)$ for every vector $u$ in the space*.

Let $A$ be a 2 by 2 matrix that multiplies all vectors in $R^2$, and let $H$ be a matrix representing 11 points that forms a house. These $v$ vectors are *transformed into $Av$ vectors*. The transformation from house to house is linear:

#+DOWNLOADED: screenshot @ 2023-05-01 10:42:45
[[file:Linearity_of_transformations/2023-05-01_10-42-45_screenshot.png]]

* Linearity on calculus
:PROPERTIES:
:ID:       b4ae3917-fd65-4411-895a-6b5e98b9252e
:END:

All of [[id:5e47e47f-f4cc-4c02-9d9b-466fcf558b78][calculus]] depends on linearity. Precalculus finds a few key derivatives, for $x^n$ and $\sin x$ and $\cos x$ and $e^x$. Then linearity applies to all their combinations.

You can use linearity for the derivative of any combination:

#+DOWNLOADED: screenshot @ 2023-05-01 09:22:12
[[file:Linearity_on_calculus/2023-05-01_09-22-12_screenshot.png]]

In the nullspace of $T(u)$, *we are solving $T(u) = 0$*, all multiple of the special solution $u = 1$.

In the column space, since the input space contains all quadratics $a + bx + cx^2$, *the outputs (column space) are linear functions $b + 2cx$*. The counting theorem is still true: $r + (n - r) = n$.


#+DOWNLOADED: screenshot @ 2023-05-01 09:47:29
[[file:Linearity_on_calculus/2023-05-01_09-47-29_screenshot.png]]

** Integration by pseudoinverse
:PROPERTIES:
:ID:       4245c845-f995-4e58-b8c3-27602ae765e1
:END:

The Fundamental Theorem of Calculus says that integration is the *(pseudo)inverse of differentiation*.

For linear algebra, the *matrix $A^+$ is the [[id:0bf225a8-bf15-4bdd-84ad-b0bbedb56183][pseudoinverse]] of the matrix $A$*, it represents the integral matrix for the integral transformation:

#+DOWNLOADED: screenshot @ 2023-05-01 10:16:13
[[file:Linearity_on_calculus/2023-05-01_10-16-13_screenshot.png]]

* Matrix of a linear transformation
:PROPERTIES:
:ID:       13b331cf-aeb8-426f-84b6-b7d8d0e395a7
:END:

For ordinary column vectors, the input $v$ is in $V = R^n$ and the output $T(v)$ is in $W = R^m$.

Every $v$ is a unique combination $c_1 v_1 + \cdots + c_n v_n$ of the *basis vectors* $v_j$. Since $T$ is a linear transformation, *$T(v)$ must be the same combination $c_1 T(v_1) + \cdots + c_n T(v_n)$ of the outputs $T(v_j)$ in the columns.*

** Change of basis

All vector spaces $V$ and $W$ have bases. When the input basis is in the columns of a matrix $V$, and the output basis is in the columns of $W$, *the change of basis matrix for $T = I$ is $B = W^{-1}V$*. The change of basis matrix $B = W^{-1} V = B^{-1}_{out} B_{in}$ represents the identity $T(v) = v$.

When the input basis is *different from the output basis*, the matrix for $T(v) = v$  will not be the identity $I$. It will be *the "change of basis matrix"*.

We choose a *basis* $v_1, \dots, v_n$ for $V$ and a *basis* $w_1, \dots, w_m$ for $W$. *The matrix $A$ will be $m$ by $n$*. To find the first column of $A$, apply $T$ to the first basis vector $v_1$. The output $T(v_1)$ is in $W$.

*$T(v_1)$ is a combination $a_{11}w_1 + \cdots + a_{m1}w_m$ of the output basis for $W$*. Transforming $v_1$ to $T(v_1)$ matches multiplying $(1,0,\dots ,0)$ by $A$. See examples below.

** Examples

#+DOWNLOADED: screenshot @ 2023-05-01 16:20:05
[[file:Matrix_of_a_linear_transformation/2023-05-01_16-20-05_screenshot.png]]

An example of *change of basis*:

#+DOWNLOADED: screenshot @ 2023-05-01 16:28:47
[[file:Matrix_of_a_linear_transformation/2023-05-01_16-28-47_screenshot.png]]

In the example above, we are using $w_1,w_2$ to represent $v_1,v_2$.

Finding the matrix for the derivative $T$:

#+DOWNLOADED: screenshot @ 2023-05-01 20:12:05
[[file:Matrix_of_a_linear_transformation/2023-05-01_20-12-05_screenshot.png]]

For the integral:

#+DOWNLOADED: screenshot @ 2023-05-01 20:12:28
[[file:Matrix_of_a_linear_transformation/2023-05-01_20-12-28_screenshot.png]]

** Products of transformations

*The product of transformations $TS$ matches products $ABx$.* The output $S(u)$ becomes the input to $T$. The output $Bx$ becomes the input to $A$.

The matrix $B$ from transformation $S$ uses $u_1, \dots, u_p$ for $U$ and a basis $v_1, \dots, v_2$ for $V$. The matrix $A$ from transformation $T$ uses same basis $v_1, \dots ,v_n$ for $V$.

$TS: U \rightarrow V \rightarrow W$

*** Examples

[[file:Matrix_of_a_linear_transformation/2023-05-04_09-09-40_screenshot.png]]

** Best bases
:PROPERTIES:
:ID:       da0d6e1b-317a-4ada-8178-199248f332fd
:END:

The standard basis vectors for $R^n$ and $R^m$ are the columns of $I$. That choice leads to a standard matrix, then $T(v) = Av$. The goal is to choose the *bases that give the best matrix (a diagonal matrix) for $T$*. The choice of best bases are bases which *diagonalize* the matrix.

With the standard basis (the columns of $I$), our transformation $T$ produces some matrix $A$, probably not diagonal. The two great choices are *eigenvectors and singular vectors*.

*** Eigenvectors
:PROPERTIES:
:ID:       66851b8e-8810-456f-976a-cd537c18e4e8
:END:
*If $T$ transforms $R^n$ to $R^n$, its matrix is square*.

If there are $n$ *independent eigenvectors*, choose those as the input and the output basis. In this good basis, the matrix for $T$ is the *diagonal eigenvalue matrix $\Lambda$*.

Notice that this choice requires $A$ to be a *square matrix and diagonalizable* ($n$ independent eigenvectors).

$B^{-1} AB = \Lambda = $ eigenvalues

*** Singular vectors
:PROPERTIES:
:ID:       60d6dbef-bdb4-4bca-93ce-25a748be4b27
:END:

We always can choose $v$'s and $w$'s that produce a diagonal matrix. This will be the *singular matrix $\Sigma$ in the [[id:b89be7e6-a282-4b57-aed3-3522ed7ff581][SVD]] decomposition $A = U \Sigma V^T$*.

The right singular vectors $v$ in $\Sigma = U^{-1} AV$ will be the *input basis*.

The left singular vectors $u$ will be the *output basis*.

In short words, the best input-output bases are eigenvectors and/or singular vectors of $A$. Then:

$\space B^{-1}_{out} AB_{in} = \Sigma = $ singular values.

*** Generalized eigenvectors
:PROPERTIES:
:ID:       ec5d86ca-7234-4be0-b7ea-fa2e8d86c92f
:END:

A is a square matrix but it *may only have $s$ independent eigenvectors* (if $s = n$ then $B$ is $X$ and $J$ is $\Lambda$).

$B_{in} = B_{out} = $ generalized eigenvectors of $A$. *Then $B^{-1}AB = J$.*

The point of [[id:15c0f5c9-bde3-4034-b887-52a0f3911790][Jordan's]] theorem is that *every square matrix $A$ has a complete set of eigenvectors and generalized eigenvectors ($n - s$)*. The good case is when the jordan matrix has 1x1 blocks.

The jordan matrix is formed by:

- $s$ square blocks along the diagonal of $J$;
- each block has one eigenvalue, one eigenvector, and 1's above the diagonal

#+DOWNLOADED: screenshot @ 2023-05-04 15:44:36
[[file:Matrix_of_a_linear_transformation/2023-05-04_15-44-36_screenshot.png]]
#+DOWNLOADED: screenshot @ 2023-05-04 15:53:27
[[file:Matrix_of_a_linear_transformation/2023-05-04_15-53-27_screenshot.png]]

*** Applications
:PROPERTIES:
:ID:       7364df22-b46c-4195-8c45-311526b8b722
:END:

The reason of why to choose the best bases is to get a diagonal (or nearly diagonal) matrix $A$. Common applications in applied mathematics have $B^{-1} AB$ close to a diagonal matrix.

An example of a [[id:8bbaa36a-9222-4d7d-a540-044b011b6371][discrete fourier transform]] $B_{in} = B_{out} = $ Fourier matrix $F$:

#+DOWNLOADED: screenshot @ 2023-05-08 08:44:15
[[file:Matrix_of_a_linear_transformation/2023-05-08_08-44-15_screenshot.png]]


*** Examples

Example of eigenvectors as bases:

#+DOWNLOADED: screenshot @ 2023-05-04 09:50:07
[[file:Matrix_of_a_linear_transformation/2023-05-04_09-50-07_screenshot.png]]

Example of singular vectors as bases (matrix will be $\Sigma$):


#+DOWNLOADED: screenshot @ 2023-05-04 10:17:55
[[file:Matrix_of_a_linear_transformation/2023-05-04_10-17-55_screenshot.png]]

*** Function space bases
:PROPERTIES:
:ID:       56d88306-20ec-4d7b-b084-69c94df0492f
:END:

When using vectors, the *test for a good basis* would be $B^TB$. The basis is orthonormal when $B^TB = I$.

When using functions, *the columns of $B$ are functions*. We still use $B^TB$ to test for independence.

The inner product (dot product when using vectors) of functions will *integrate* instead of adding.

**** Inner product $(f,g)$

$\int f(x) \space g(x) dx$

**** Complex inner product $(f,g)$

$\int \overline{f(x)} \space g(x) dx$.

$\overline{f} = $ complex conjugate

**** Weighted inner product $(f,g)_w$

$\int w(x) \space \overline{f(x)} \space g(x) dx$

$w = $ weight function

**** Orthogonal bases for function space
:PROPERTIES:
:ID:       b74e0304-2253-4303-9d9d-c7dd907c51e8
:END:

Orthogonal bases for function spaces are used for *better computation*.

#+DOWNLOADED: screenshot @ 2023-05-08 10:41:12
[[file:Matrix_of_a_linear_transformation/2023-05-08_10-41-12_screenshot.png]]


***** Fourier basis
:PROPERTIES:
:ID:       958ceb10-4c6c-4676-ba85-755fa0a4bfbf
:END:

The fourier basis is $1,\sin x,\cos x,\sin 2x,\cos 2x,\dots$.

This basis is specially good for *functions $f(x)$ that are themselves periodic*: $f(x + 2 \pi) = f(x)$.

The fourier *transform connects* $f(x)$ to the *coefficients $a_k$ and $b_k$* in its Fourier series:

$f(x) = a_0 + b_1 \sin x + a_1 \cos x + b_2 \sin 2x + a_2 \cos 2x + \cdots$

The *function space is infinite dimensional*, it takes infinitely many basis functions to capture perfectly a typical $f(x)$.

The formula for each coefficient is *similar* to the formula $b^Ta / a^Ta$ for *projecting a vector $b$ onto the line through $a$*:

#+DOWNLOADED: screenshot @ 2023-05-08 10:09:06
[[file:Matrix_of_a_linear_transformation/2023-05-08_10-09-06_screenshot.png]]

***** Legendre basis
:PROPERTIES:
:ID:       8cf4849b-de87-4141-beb3-eb020dc84056
:END:

The [[id:44a664a1-b6b3-446c-b6c7-2d5e6cfd8a0d][legendre basis]] are the result of applying the *[[id:61455b3d-6cb7-4499-8e49-e2fc552884a1][Gram Schmidt]] idea*. *We orthogonalize the powers $1,x,x^2,\dots$*:

$1,x,x^2 - \frac{1}{3},x^3 - \frac{3}{5}x, \dots$

#+DOWNLOADED: screenshot @ 2023-05-08 10:17:04
[[file:Matrix_of_a_linear_transformation/2023-05-08_10-17-04_screenshot.png]]

***** Chebyshev basis
:PROPERTIES:
:ID:       d6eba86a-a7dd-4f81-b505-d70afd0f5baf
:END:

The [[id:8b170316-dfa7-44b3-b71e-0eaf3558fdac][chebyshev basis]] is the polynomial $1,x,2x^2 - 1,4x^3 - 3x, \dots$.

The $n^{th}$ degree Chebyshev polynomial $T_n(x)$ *converts to Fourier's* $\cos n\theta = T_n(\cos \theta)$.
